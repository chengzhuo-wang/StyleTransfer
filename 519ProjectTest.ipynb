{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "510ProjectTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_vhyVfK4jmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "08c6de53-35ee-418c-81f1-b26960da0679"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ4HveagQ-IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datas\n",
        "import torchvision.transforms as tf\n",
        "import torchvision.utils as tutils\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "import torchvision.models.vgg as VGG"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owZw85Juwwnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, pooling):\n",
        "        super(VGG,self).__init__()\n",
        "        # VGG structures\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.p1 = pooling(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.p2 = pooling(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.p3 = pooling(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.p4 = pooling(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.p5 = pooling(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, out_params = None):\n",
        "        out = {}\n",
        "        # Building up the VGG net that's going to be used\n",
        "        out['re11'] = F.relu(self.conv1_1(x))\n",
        "        out['re12'] = F.relu(self.conv1_2(out['re11']))\n",
        "        out['p1'] = self.p1(out['re12'])\n",
        "        h_relu1_2 = out['re12']\n",
        "        out['re21'] = F.relu(self.conv2_1(out['p1']))\n",
        "        out['re22'] = F.relu(self.conv2_2(out['re21']))\n",
        "        out['p2'] = self.p2(out['re22'])\n",
        "        h_relu2_2 = out['re22']\n",
        "        out['re31'] = F.relu(self.conv3_1(out['p2']))\n",
        "        out['re32'] = F.relu(self.conv3_2(out['re31']))\n",
        "        out['re33'] = F.relu(self.conv3_3(out['re32']))\n",
        "        out['re34'] = F.relu(self.conv3_4(out['re33']))\n",
        "        out['p3'] = self.p3(out['re34'])\n",
        "        h_relu3_3 = out['re33']\n",
        "        out['re41'] = F.relu(self.conv4_1(out['p3']))\n",
        "        out['re42'] = F.relu(self.conv4_2(out['re41']))\n",
        "        out['re43'] = F.relu(self.conv4_3(out['re42']))\n",
        "        out['re44'] = F.relu(self.conv4_4(out['re43']))\n",
        "        h_relu4_3 = out['re43']\n",
        "        out['p4'] = self.p4(out['re44'])\n",
        "        out['re51'] = F.relu(self.conv5_1(out['p4']))\n",
        "        out['re52'] = F.relu(self.conv5_2(out['re51']))\n",
        "        out['re53'] = F.relu(self.conv5_3(out['re52']))\n",
        "        out['re54'] = F.relu(self.conv5_4(out['re53']))\n",
        "        out['p5'] = self.p5(out['re54'])\n",
        "        if out_params is not None:\n",
        "             return [out[param] for param in out_params]\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkzkcPadLcWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, input):\n",
        "        b, c, h, w = input.size()\n",
        "        f = input.view(b, c, h*w) #bxcx(hxw)\n",
        "        # torch.bmm(batch1, batch2, out=None)\n",
        "        # batch1 : bxmxp, batch2 : bxpxn -> bxmxn\n",
        "        G = torch.bmm(f, f.transpose(1, 2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
        "        return G.div_(h*w)\n",
        "\n",
        "class styleLoss(nn.Module):\n",
        "    def forward(self, input, target):\n",
        "        GramInput = GramMatrix()(input)\n",
        "        return nn.MSELoss()(GramInput, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPlJfpehLrux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dataset Processing\n",
        "transform = tf.Compose([\n",
        "    tf.Resize(512), #Default image_size\n",
        "    #tf.Grayscale(num_output_channels=1),\n",
        "    tf.ToTensor(), #Transform it to a torch tensor\n",
        "    tf.Lambda(lambda x:x[torch.LongTensor([2, 1, 0])]), #Converting from RGB to BGR\n",
        "    tf.Normalize(mean=[0.333, 0.333, 0.333], std=[0.5,0.5,0.5]), #subracting imagenet mean\n",
        "    tf.Lambda(lambda x: x.mul_(255))\n",
        "    ])\n",
        "\n",
        "def load_img(path):\n",
        "    img = Image.open(path)\n",
        "    img = Variable(transform(img))\n",
        "    img = img.unsqueeze(0)\n",
        "    return img\n",
        "\n",
        "def save_img(img):\n",
        "    post = tf.Compose([\n",
        "         tf.Lambda(lambda x: x.mul_(1./255)),\n",
        "         tf.Normalize(mean=[-0.333, -0.333, -0.333], std=[1,1,1]),\n",
        "         tf.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
        "         ])\n",
        "    img = post(img)\n",
        "    img = img.clamp_(0,1)\n",
        "    tutils.save_image(img,\n",
        "                '%s/transfer_LBFGS_30_5cl_2.png' % (\"./images\"),\n",
        "                normalize=True)\n",
        "    return\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()\n",
        "    out = vgg(optimImg, loss_layers)\n",
        "    totalLossList = []\n",
        "    for i in range(len(out)):\n",
        "        layer_output = out[i]\n",
        "        loss_i = losses[i]\n",
        "        target_i = targets[i]\n",
        "        totalLoss = loss_i(layer_output, target_i) * weights[i]\n",
        "        totalLossList.append(totalLoss)\n",
        "    totalLoss = sum(totalLossList)\n",
        "    totalLoss.backward()\n",
        "    print('Loss: %f'%(totalLoss.data))\n",
        "    return totalLoss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bca7kK3TL5qY",
        "colab_type": "code",
        "outputId": "1a347512-db0b-48a6-9ec0-5023748f1dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vgg_directory = \"./drive/My Drive/UPenn_Courses/Fall2019/MachineLearning/Project/Images/vgg_conv.pth\" #path to pretrained vgg vgg_directory\n",
        "vgg = VGG(pooling = nn.AvgPool2d)\n",
        "#print(vgg.state_dict())\n",
        "vgg.load_state_dict(torch.load(vgg_directory))\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "vgg.cuda() # Putting model on cuda\n",
        "\n",
        "style_img = \"./drive/My Drive/UPenn_Courses/Fall2019/MachineLearning/Project/Images/xingkong.jpg\"\n",
        "content_img = \"./drive/My Drive/UPenn_Courses/Fall2019/MachineLearning/Project/Images/paris2.jpg\"\n",
        "styleImg = load_img(style_img)\n",
        "contentImg = load_img(content_img)\n",
        "styleImg = styleImg.cuda()\n",
        "contentImg = contentImg.cuda()\n",
        "\n",
        "style_weight = 1\n",
        "content_weight = 9\n",
        "content_layers = ['re12', 're22', 're32', 're42', 're52']\n",
        "style_layers = ['re11','re21','re31','re41','re51']\n",
        "style_Losses = [styleLoss()] * len(style_layers)\n",
        "content_Losses = [nn.MSELoss()] * len(content_layers)\n",
        "\n",
        "styleTargets = []\n",
        "for t in vgg(styleImg, style_layers):\n",
        "    t = t.detach()\n",
        "    styleTargets.append(GramMatrix()(t))\n",
        "\n",
        "contentTargets = []\n",
        "for t in vgg(contentImg, content_layers):\n",
        "    t = t.detach()\n",
        "    contentTargets.append(t)\n",
        "\n",
        "losses = style_Losses + content_Losses\n",
        "targets = styleTargets + contentTargets\n",
        "loss_layers = style_layers + content_layers\n",
        "weights = [style_weight] * len(style_layers) + [content_weight] * len(content_layers)\n",
        "\n",
        "\n",
        "#setting seed for pytorch\n",
        "#torch.cuda.manual_seed_all(random.randint(1, 10000))\n",
        "torch.cuda.manual_seed_all(1218)\n",
        "if not os.path.exists(\"images/\"):\n",
        "    os.makedirs(\"images/\")\n",
        "#The below flag allows you to enable the cudnn auto-tuner\n",
        "#to find the best algorithm for your hardware\n",
        "cudnn.benchmark = True\n",
        "\n",
        "\n",
        "optimImg = Variable(contentImg.data.clone(), requires_grad=True)\n",
        "optimizer = optim.LBFGS([optimImg], lr=1, max_iter=30)\n",
        "#optimizer = optim.SGD([optimImg], lr = 0.001, momentum=0.9)\n",
        "\n",
        "#Shifting everything to cuda\n",
        "for loss in losses:\n",
        "    loss = loss.cuda()\n",
        "optimImg.cuda()\n",
        "\n",
        "# Training\n",
        "epoches = 30\n",
        "\n",
        "for epoch in range(epoches):\n",
        "    print('Epoch: {}/{}'.format(epoch + 1, epoches))\n",
        "    optimizer.step(closure)\n",
        "print('Style transfer is completed')\n",
        "outImg = optimImg.data[0].cpu()\n",
        "save_img(outImg.squeeze())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30\n",
            "Loss: 124752502784.000000\n",
            "Loss: 124752502784.000000\n",
            "Epoch: 2/30\n",
            "Loss: 124752502784.000000\n",
            "Loss: 19108452161537703936.000000\n",
            "Loss: 124339101696.000000\n",
            "Loss: 123966160896.000000\n",
            "Loss: 123585486848.000000\n",
            "Loss: 123198431232.000000\n",
            "Loss: 122807205888.000000\n",
            "Loss: 122413023232.000000\n",
            "Loss: 122017513472.000000\n",
            "Loss: 121621987328.000000\n",
            "Loss: 69818916864.000000\n",
            "Loss: 35102433280.000000\n",
            "Loss: 20536731648.000000\n",
            "Loss: 13926533120.000000\n",
            "Loss: 10822856704.000000\n",
            "Loss: 7230490112.000000\n",
            "Loss: 5204071936.000000\n",
            "Loss: 3806949632.000000\n",
            "Loss: 3140289536.000000\n",
            "Loss: 2351813120.000000\n",
            "Loss: 2185562368.000000\n",
            "Loss: 2003401344.000000\n",
            "Loss: 1663170176.000000\n",
            "Loss: 1392043136.000000\n",
            "Loss: 1178405504.000000\n",
            "Loss: 999624320.000000\n",
            "Loss: 913091200.000000\n",
            "Loss: 811279936.000000\n",
            "Loss: 748297664.000000\n",
            "Loss: 691478400.000000\n",
            "Epoch: 3/30\n",
            "Loss: 638517376.000000\n",
            "Loss: 571293696.000000\n",
            "Loss: 531270112.000000\n",
            "Loss: 507755424.000000\n",
            "Loss: 462096192.000000\n",
            "Loss: 438166496.000000\n",
            "Loss: 439088480.000000\n",
            "Loss: 386311008.000000\n",
            "Loss: 373664320.000000\n",
            "Loss: 334178304.000000\n",
            "Loss: 350144448.000000\n",
            "Loss: 313805248.000000\n",
            "Loss: 305503040.000000\n",
            "Loss: 289950848.000000\n",
            "Loss: 269679712.000000\n",
            "Loss: 260193312.000000\n",
            "Loss: 238644160.000000\n",
            "Loss: 234348080.000000\n",
            "Loss: 225640784.000000\n",
            "Loss: 218099840.000000\n",
            "Loss: 208011600.000000\n",
            "Loss: 198075904.000000\n",
            "Loss: 190153632.000000\n",
            "Loss: 183348304.000000\n",
            "Loss: 178349168.000000\n",
            "Loss: 172992928.000000\n",
            "Loss: 166941152.000000\n",
            "Loss: 163565952.000000\n",
            "Loss: 154194288.000000\n",
            "Loss: 159496800.000000\n",
            "Epoch: 4/30\n",
            "Loss: 143865968.000000\n",
            "Loss: 140918256.000000\n",
            "Loss: 137436096.000000\n",
            "Loss: 133802824.000000\n",
            "Loss: 128189616.000000\n",
            "Loss: 124030416.000000\n",
            "Loss: 120786464.000000\n",
            "Loss: 118844024.000000\n",
            "Loss: 114933440.000000\n",
            "Loss: 111612768.000000\n",
            "Loss: 108894824.000000\n",
            "Loss: 106236920.000000\n",
            "Loss: 102638408.000000\n",
            "Loss: 99235336.000000\n",
            "Loss: 96560984.000000\n",
            "Loss: 95255680.000000\n",
            "Loss: 93376640.000000\n",
            "Loss: 91006416.000000\n",
            "Loss: 88430664.000000\n",
            "Loss: 86181712.000000\n",
            "Loss: 83974776.000000\n",
            "Loss: 81320552.000000\n",
            "Loss: 79632192.000000\n",
            "Loss: 77536416.000000\n",
            "Loss: 76068408.000000\n",
            "Loss: 74918424.000000\n",
            "Loss: 73059352.000000\n",
            "Loss: 71372112.000000\n",
            "Loss: 70253296.000000\n",
            "Loss: 68384520.000000\n",
            "Epoch: 5/30\n",
            "Loss: 66480272.000000\n",
            "Loss: 64934620.000000\n",
            "Loss: 63088972.000000\n",
            "Loss: 62492140.000000\n",
            "Loss: 60127024.000000\n",
            "Loss: 59092664.000000\n",
            "Loss: 57986748.000000\n",
            "Loss: 56820232.000000\n",
            "Loss: 54572612.000000\n",
            "Loss: 53244692.000000\n",
            "Loss: 52208048.000000\n",
            "Loss: 51820784.000000\n",
            "Loss: 50898160.000000\n",
            "Loss: 50127264.000000\n",
            "Loss: 48128244.000000\n",
            "Loss: 47079468.000000\n",
            "Loss: 46171560.000000\n",
            "Loss: 45568872.000000\n",
            "Loss: 44590660.000000\n",
            "Loss: 43690536.000000\n",
            "Loss: 42562976.000000\n",
            "Loss: 41889328.000000\n",
            "Loss: 41452196.000000\n",
            "Loss: 40452888.000000\n",
            "Loss: 39974136.000000\n",
            "Loss: 39395112.000000\n",
            "Loss: 38757068.000000\n",
            "Loss: 38661632.000000\n",
            "Loss: 38118824.000000\n",
            "Loss: 37643744.000000\n",
            "Epoch: 6/30\n",
            "Loss: 37123032.000000\n",
            "Loss: 36223252.000000\n",
            "Loss: 35657400.000000\n",
            "Loss: 35009008.000000\n",
            "Loss: 34577108.000000\n",
            "Loss: 34372336.000000\n",
            "Loss: 33763768.000000\n",
            "Loss: 33418766.000000\n",
            "Loss: 32677708.000000\n",
            "Loss: 32189254.000000\n",
            "Loss: 31702630.000000\n",
            "Loss: 31114078.000000\n",
            "Loss: 30750522.000000\n",
            "Loss: 30181186.000000\n",
            "Loss: 29628356.000000\n",
            "Loss: 29101940.000000\n",
            "Loss: 28768566.000000\n",
            "Loss: 28529210.000000\n",
            "Loss: 28129172.000000\n",
            "Loss: 27846608.000000\n",
            "Loss: 27442434.000000\n",
            "Loss: 26943370.000000\n",
            "Loss: 26491662.000000\n",
            "Loss: 26224268.000000\n",
            "Loss: 25944640.000000\n",
            "Loss: 25652612.000000\n",
            "Loss: 25380734.000000\n",
            "Loss: 25103074.000000\n",
            "Loss: 24879794.000000\n",
            "Loss: 24371314.000000\n",
            "Epoch: 7/30\n",
            "Loss: 24093300.000000\n",
            "Loss: 23939374.000000\n",
            "Loss: 23652110.000000\n",
            "Loss: 23538248.000000\n",
            "Loss: 23248284.000000\n",
            "Loss: 23044640.000000\n",
            "Loss: 22903218.000000\n",
            "Loss: 22634362.000000\n",
            "Loss: 22271776.000000\n",
            "Loss: 22002642.000000\n",
            "Loss: 21849324.000000\n",
            "Loss: 21680044.000000\n",
            "Loss: 21486738.000000\n",
            "Loss: 21234216.000000\n",
            "Loss: 21044562.000000\n",
            "Loss: 20888738.000000\n",
            "Loss: 20757786.000000\n",
            "Loss: 20632412.000000\n",
            "Loss: 20479444.000000\n",
            "Loss: 20253492.000000\n",
            "Loss: 20112554.000000\n",
            "Loss: 20034022.000000\n",
            "Loss: 19885850.000000\n",
            "Loss: 19710610.000000\n",
            "Loss: 19557608.000000\n",
            "Loss: 19445810.000000\n",
            "Loss: 19332914.000000\n",
            "Loss: 19173576.000000\n",
            "Loss: 18979516.000000\n",
            "Loss: 18846080.000000\n",
            "Epoch: 8/30\n",
            "Loss: 18711012.000000\n",
            "Loss: 18560922.000000\n",
            "Loss: 18392162.000000\n",
            "Loss: 18275932.000000\n",
            "Loss: 18176628.000000\n",
            "Loss: 18064638.000000\n",
            "Loss: 17898206.000000\n",
            "Loss: 17801002.000000\n",
            "Loss: 17719168.000000\n",
            "Loss: 17594382.000000\n",
            "Loss: 17507004.000000\n",
            "Loss: 17392308.000000\n",
            "Loss: 17286856.000000\n",
            "Loss: 17199210.000000\n",
            "Loss: 17124942.000000\n",
            "Loss: 16988194.000000\n",
            "Loss: 16909488.000000\n",
            "Loss: 16856682.000000\n",
            "Loss: 16731901.000000\n",
            "Loss: 16665269.000000\n",
            "Loss: 16581745.000000\n",
            "Loss: 16511469.000000\n",
            "Loss: 16444920.000000\n",
            "Loss: 16347203.000000\n",
            "Loss: 16241077.000000\n",
            "Loss: 16166706.000000\n",
            "Loss: 16112774.000000\n",
            "Loss: 16043042.000000\n",
            "Loss: 15945641.000000\n",
            "Loss: 15858141.000000\n",
            "Epoch: 9/30\n",
            "Loss: 15795878.000000\n",
            "Loss: 15715987.000000\n",
            "Loss: 15637230.000000\n",
            "Loss: 15547201.000000\n",
            "Loss: 15472114.000000\n",
            "Loss: 15406956.000000\n",
            "Loss: 15348656.000000\n",
            "Loss: 15272389.000000\n",
            "Loss: 15250453.000000\n",
            "Loss: 15176802.000000\n",
            "Loss: 15144611.000000\n",
            "Loss: 15065103.000000\n",
            "Loss: 14989726.000000\n",
            "Loss: 14925834.000000\n",
            "Loss: 14872030.000000\n",
            "Loss: 14827170.000000\n",
            "Loss: 14747002.000000\n",
            "Loss: 14685432.000000\n",
            "Loss: 14628354.000000\n",
            "Loss: 14589648.000000\n",
            "Loss: 14560942.000000\n",
            "Loss: 14510376.000000\n",
            "Loss: 14459262.000000\n",
            "Loss: 14404144.000000\n",
            "Loss: 14365197.000000\n",
            "Loss: 14333983.000000\n",
            "Loss: 14248351.000000\n",
            "Loss: 14185223.000000\n",
            "Loss: 14154881.000000\n",
            "Loss: 14125705.000000\n",
            "Epoch: 10/30\n",
            "Loss: 14098623.000000\n",
            "Loss: 14075325.000000\n",
            "Loss: 14054083.000000\n",
            "Loss: 14007564.000000\n",
            "Loss: 13918635.000000\n",
            "Loss: 13856116.000000\n",
            "Loss: 13819385.000000\n",
            "Loss: 13791828.000000\n",
            "Loss: 13764706.000000\n",
            "Loss: 13725405.000000\n",
            "Loss: 13660011.000000\n",
            "Loss: 13618321.000000\n",
            "Loss: 13587176.000000\n",
            "Loss: 13560982.000000\n",
            "Loss: 13535258.000000\n",
            "Loss: 13486548.000000\n",
            "Loss: 13445787.000000\n",
            "Loss: 13409868.000000\n",
            "Loss: 13377572.000000\n",
            "Loss: 13347702.000000\n",
            "Loss: 13309657.000000\n",
            "Loss: 13279762.000000\n",
            "Loss: 13254560.000000\n",
            "Loss: 13223905.000000\n",
            "Loss: 13184884.000000\n",
            "Loss: 13144973.000000\n",
            "Loss: 13118897.000000\n",
            "Loss: 13094405.000000\n",
            "Loss: 13060269.000000\n",
            "Loss: 13029535.000000\n",
            "Epoch: 11/30\n",
            "Loss: 13003899.000000\n",
            "Loss: 12980669.000000\n",
            "Loss: 12959774.000000\n",
            "Loss: 12911196.000000\n",
            "Loss: 12885153.000000\n",
            "Loss: 12863907.000000\n",
            "Loss: 12836309.000000\n",
            "Loss: 12805728.000000\n",
            "Loss: 12777511.000000\n",
            "Loss: 12747987.000000\n",
            "Loss: 12723685.000000\n",
            "Loss: 12699330.000000\n",
            "Loss: 12668227.000000\n",
            "Loss: 12642537.000000\n",
            "Loss: 12624325.000000\n",
            "Loss: 12600264.000000\n",
            "Loss: 12578955.000000\n",
            "Loss: 12548292.000000\n",
            "Loss: 12521739.000000\n",
            "Loss: 12492739.000000\n",
            "Loss: 12462815.000000\n",
            "Loss: 12436490.000000\n",
            "Loss: 12418615.000000\n",
            "Loss: 12401045.000000\n",
            "Loss: 12393114.000000\n",
            "Loss: 12369428.000000\n",
            "Loss: 12354414.000000\n",
            "Loss: 12330786.000000\n",
            "Loss: 12296521.000000\n",
            "Loss: 12264912.000000\n",
            "Epoch: 12/30\n",
            "Loss: 12251073.000000\n",
            "Loss: 12237412.000000\n",
            "Loss: 12208436.000000\n",
            "Loss: 12186146.000000\n",
            "Loss: 12171461.000000\n",
            "Loss: 12160682.000000\n",
            "Loss: 12150361.000000\n",
            "Loss: 12137930.000000\n",
            "Loss: 12113702.000000\n",
            "Loss: 12101227.000000\n",
            "Loss: 12063092.000000\n",
            "Loss: 12053062.000000\n",
            "Loss: 12032552.000000\n",
            "Loss: 12020245.000000\n",
            "Loss: 12006341.000000\n",
            "Loss: 11990129.000000\n",
            "Loss: 11969790.000000\n",
            "Loss: 11950585.000000\n",
            "Loss: 11923893.000000\n",
            "Loss: 11903693.000000\n",
            "Loss: 11892684.000000\n",
            "Loss: 11880147.000000\n",
            "Loss: 11867431.000000\n",
            "Loss: 11848326.000000\n",
            "Loss: 11830811.000000\n",
            "Loss: 11811404.000000\n",
            "Loss: 11793292.000000\n",
            "Loss: 11776052.000000\n",
            "Loss: 11759129.000000\n",
            "Loss: 11744312.000000\n",
            "Epoch: 13/30\n",
            "Loss: 11732807.000000\n",
            "Loss: 11724359.000000\n",
            "Loss: 11709601.000000\n",
            "Loss: 11697472.000000\n",
            "Loss: 11683319.000000\n",
            "Loss: 11662073.000000\n",
            "Loss: 11646612.000000\n",
            "Loss: 11635921.000000\n",
            "Loss: 11617266.000000\n",
            "Loss: 11599581.000000\n",
            "Loss: 11588104.000000\n",
            "Loss: 11580687.000000\n",
            "Loss: 11573619.000000\n",
            "Loss: 11563115.000000\n",
            "Loss: 11541789.000000\n",
            "Loss: 11530433.000000\n",
            "Loss: 11513318.000000\n",
            "Loss: 11496537.000000\n",
            "Loss: 11479693.000000\n",
            "Loss: 11469594.000000\n",
            "Loss: 11454587.000000\n",
            "Loss: 11439142.000000\n",
            "Loss: 11425958.000000\n",
            "Loss: 11416048.000000\n",
            "Loss: 11407776.000000\n",
            "Loss: 11398500.000000\n",
            "Loss: 11383919.000000\n",
            "Loss: 11370126.000000\n",
            "Loss: 11357259.000000\n",
            "Loss: 11348578.000000\n",
            "Epoch: 14/30\n",
            "Loss: 11336943.000000\n",
            "Loss: 11320191.000000\n",
            "Loss: 11307933.000000\n",
            "Loss: 11299214.000000\n",
            "Loss: 11290619.000000\n",
            "Loss: 11284421.000000\n",
            "Loss: 11267887.000000\n",
            "Loss: 11261424.000000\n",
            "Loss: 11251148.000000\n",
            "Loss: 11246215.000000\n",
            "Loss: 11239339.000000\n",
            "Loss: 11227775.000000\n",
            "Loss: 11212990.000000\n",
            "Loss: 11201651.000000\n",
            "Loss: 11192369.000000\n",
            "Loss: 11180331.000000\n",
            "Loss: 11167110.000000\n",
            "Loss: 11155751.000000\n",
            "Loss: 11147105.000000\n",
            "Loss: 11143255.000000\n",
            "Loss: 11133133.000000\n",
            "Loss: 11126368.000000\n",
            "Loss: 11118974.000000\n",
            "Loss: 11104589.000000\n",
            "Loss: 11087196.000000\n",
            "Loss: 11074330.000000\n",
            "Loss: 11066855.000000\n",
            "Loss: 11058293.000000\n",
            "Loss: 11049366.000000\n",
            "Loss: 11040018.000000\n",
            "Epoch: 15/30\n",
            "Loss: 11033124.000000\n",
            "Loss: 11026518.000000\n",
            "Loss: 11020952.000000\n",
            "Loss: 11011552.000000\n",
            "Loss: 10998877.000000\n",
            "Loss: 10988356.000000\n",
            "Loss: 10982523.000000\n",
            "Loss: 10976155.000000\n",
            "Loss: 10966860.000000\n",
            "Loss: 10958316.000000\n",
            "Loss: 10952190.000000\n",
            "Loss: 10945867.000000\n",
            "Loss: 10938769.000000\n",
            "Loss: 10928836.000000\n",
            "Loss: 10917153.000000\n",
            "Loss: 10908466.000000\n",
            "Loss: 10903061.000000\n",
            "Loss: 10892872.000000\n",
            "Loss: 10884363.000000\n",
            "Loss: 10877048.000000\n",
            "Loss: 10872496.000000\n",
            "Loss: 10868707.000000\n",
            "Loss: 10864407.000000\n",
            "Loss: 10852230.000000\n",
            "Loss: 10842562.000000\n",
            "Loss: 10831085.000000\n",
            "Loss: 10824395.000000\n",
            "Loss: 10820058.000000\n",
            "Loss: 10813023.000000\n",
            "Loss: 10803825.000000\n",
            "Epoch: 16/30\n",
            "Loss: 10793496.000000\n",
            "Loss: 10787593.000000\n",
            "Loss: 10783003.000000\n",
            "Loss: 10777555.000000\n",
            "Loss: 10770757.000000\n",
            "Loss: 10763874.000000\n",
            "Loss: 10758204.000000\n",
            "Loss: 10750864.000000\n",
            "Loss: 10743238.000000\n",
            "Loss: 10734454.000000\n",
            "Loss: 10727826.000000\n",
            "Loss: 10720685.000000\n",
            "Loss: 10714931.000000\n",
            "Loss: 10708918.000000\n",
            "Loss: 10702585.000000\n",
            "Loss: 10696970.000000\n",
            "Loss: 10687887.000000\n",
            "Loss: 10679235.000000\n",
            "Loss: 10674543.000000\n",
            "Loss: 10670385.000000\n",
            "Loss: 10661518.000000\n",
            "Loss: 10652692.000000\n",
            "Loss: 10647497.000000\n",
            "Loss: 10643484.000000\n",
            "Loss: 10639273.000000\n",
            "Loss: 10634398.000000\n",
            "Loss: 10624732.000000\n",
            "Loss: 10620311.000000\n",
            "Loss: 10612161.000000\n",
            "Loss: 10604531.000000\n",
            "Epoch: 17/30\n",
            "Loss: 10597187.000000\n",
            "Loss: 10592727.000000\n",
            "Loss: 10585504.000000\n",
            "Loss: 10579518.000000\n",
            "Loss: 10573279.000000\n",
            "Loss: 10568970.000000\n",
            "Loss: 10563200.000000\n",
            "Loss: 10555896.000000\n",
            "Loss: 10549815.000000\n",
            "Loss: 10543755.000000\n",
            "Loss: 10535485.000000\n",
            "Loss: 10528375.000000\n",
            "Loss: 10523853.000000\n",
            "Loss: 10519654.000000\n",
            "Loss: 10513989.000000\n",
            "Loss: 10507817.000000\n",
            "Loss: 10502442.000000\n",
            "Loss: 10499103.000000\n",
            "Loss: 10495528.000000\n",
            "Loss: 10489221.000000\n",
            "Loss: 10479857.000000\n",
            "Loss: 10472111.000000\n",
            "Loss: 10468069.000000\n",
            "Loss: 10464473.000000\n",
            "Loss: 10456938.000000\n",
            "Loss: 10453299.000000\n",
            "Loss: 10447662.000000\n",
            "Loss: 10444390.000000\n",
            "Loss: 10441594.000000\n",
            "Loss: 10436910.000000\n",
            "Epoch: 18/30\n",
            "Loss: 10432109.000000\n",
            "Loss: 10425822.000000\n",
            "Loss: 10418769.000000\n",
            "Loss: 10412796.000000\n",
            "Loss: 10408777.000000\n",
            "Loss: 10402884.000000\n",
            "Loss: 10398455.000000\n",
            "Loss: 10392411.000000\n",
            "Loss: 10389134.000000\n",
            "Loss: 10386135.000000\n",
            "Loss: 10383462.000000\n",
            "Loss: 10378195.000000\n",
            "Loss: 10372241.000000\n",
            "Loss: 10366886.000000\n",
            "Loss: 10362771.000000\n",
            "Loss: 10357844.000000\n",
            "Loss: 10353975.000000\n",
            "Loss: 10350810.000000\n",
            "Loss: 10345508.000000\n",
            "Loss: 10339656.000000\n",
            "Loss: 10336360.000000\n",
            "Loss: 10333241.000000\n",
            "Loss: 10330418.000000\n",
            "Loss: 10325998.000000\n",
            "Loss: 10320381.000000\n",
            "Loss: 10315829.000000\n",
            "Loss: 10312281.000000\n",
            "Loss: 10309698.000000\n",
            "Loss: 10306188.000000\n",
            "Loss: 10301310.000000\n",
            "Epoch: 19/30\n",
            "Loss: 10297031.000000\n",
            "Loss: 10293660.000000\n",
            "Loss: 10290727.000000\n",
            "Loss: 10286419.000000\n",
            "Loss: 10282946.000000\n",
            "Loss: 10280203.000000\n",
            "Loss: 10275581.000000\n",
            "Loss: 10271268.000000\n",
            "Loss: 10267215.000000\n",
            "Loss: 10263958.000000\n",
            "Loss: 10259994.000000\n",
            "Loss: 10255243.000000\n",
            "Loss: 10251231.000000\n",
            "Loss: 10248233.000000\n",
            "Loss: 10245240.000000\n",
            "Loss: 10241872.000000\n",
            "Loss: 10236619.000000\n",
            "Loss: 10233178.000000\n",
            "Loss: 10229519.000000\n",
            "Loss: 10225680.000000\n",
            "Loss: 10221322.000000\n",
            "Loss: 10217929.000000\n",
            "Loss: 10212494.000000\n",
            "Loss: 10208983.000000\n",
            "Loss: 10205788.000000\n",
            "Loss: 10201850.000000\n",
            "Loss: 10198659.000000\n",
            "Loss: 10193955.000000\n",
            "Loss: 10190130.000000\n",
            "Loss: 10186883.000000\n",
            "Epoch: 20/30\n",
            "Loss: 10183938.000000\n",
            "Loss: 10181720.000000\n",
            "Loss: 10179138.000000\n",
            "Loss: 10176026.000000\n",
            "Loss: 10172478.000000\n",
            "Loss: 10168910.000000\n",
            "Loss: 10163977.000000\n",
            "Loss: 10159425.000000\n",
            "Loss: 10155296.000000\n",
            "Loss: 10152500.000000\n",
            "Loss: 10150163.000000\n",
            "Loss: 10147262.000000\n",
            "Loss: 10143902.000000\n",
            "Loss: 10139625.000000\n",
            "Loss: 10134181.000000\n",
            "Loss: 10129821.000000\n",
            "Loss: 10126656.000000\n",
            "Loss: 10123952.000000\n",
            "Loss: 10120873.000000\n",
            "Loss: 10115468.000000\n",
            "Loss: 10111155.000000\n",
            "Loss: 10108334.000000\n",
            "Loss: 10105553.000000\n",
            "Loss: 10101698.000000\n",
            "Loss: 10096446.000000\n",
            "Loss: 10094428.000000\n",
            "Loss: 10091998.000000\n",
            "Loss: 10090032.000000\n",
            "Loss: 10087714.000000\n",
            "Loss: 10083019.000000\n",
            "Epoch: 21/30\n",
            "Loss: 10078583.000000\n",
            "Loss: 10075099.000000\n",
            "Loss: 10072472.000000\n",
            "Loss: 10070146.000000\n",
            "Loss: 10066376.000000\n",
            "Loss: 10061655.000000\n",
            "Loss: 10058029.000000\n",
            "Loss: 10055303.000000\n",
            "Loss: 10053441.000000\n",
            "Loss: 10051082.000000\n",
            "Loss: 10047666.000000\n",
            "Loss: 10043713.000000\n",
            "Loss: 10040051.000000\n",
            "Loss: 10037581.000000\n",
            "Loss: 10034010.000000\n",
            "Loss: 10035575.000000\n",
            "Loss: 10029097.000000\n",
            "Loss: 10027425.000000\n",
            "Loss: 10025670.000000\n",
            "Loss: 10021451.000000\n",
            "Loss: 10019382.000000\n",
            "Loss: 10015862.000000\n",
            "Loss: 10013632.000000\n",
            "Loss: 10011730.000000\n",
            "Loss: 10007952.000000\n",
            "Loss: 10003254.000000\n",
            "Loss: 9999023.000000\n",
            "Loss: 9996723.000000\n",
            "Loss: 9994475.000000\n",
            "Loss: 9991957.000000\n",
            "Epoch: 22/30\n",
            "Loss: 9987574.000000\n",
            "Loss: 9985031.000000\n",
            "Loss: 9982768.000000\n",
            "Loss: 9980215.000000\n",
            "Loss: 9978394.000000\n",
            "Loss: 9975215.000000\n",
            "Loss: 9972461.000000\n",
            "Loss: 9969918.000000\n",
            "Loss: 9967118.000000\n",
            "Loss: 9965098.000000\n",
            "Loss: 9962198.000000\n",
            "Loss: 9959919.000000\n",
            "Loss: 9957732.000000\n",
            "Loss: 9954503.000000\n",
            "Loss: 9952243.000000\n",
            "Loss: 9950012.000000\n",
            "Loss: 9947848.000000\n",
            "Loss: 9945574.000000\n",
            "Loss: 9942686.000000\n",
            "Loss: 9940090.000000\n",
            "Loss: 9937350.000000\n",
            "Loss: 9934399.000000\n",
            "Loss: 9931680.000000\n",
            "Loss: 9929428.000000\n",
            "Loss: 9927187.000000\n",
            "Loss: 9924856.000000\n",
            "Loss: 9922242.000000\n",
            "Loss: 9920482.000000\n",
            "Loss: 9918471.000000\n",
            "Loss: 9916512.000000\n",
            "Epoch: 23/30\n",
            "Loss: 9913820.000000\n",
            "Loss: 9911968.000000\n",
            "Loss: 9910113.000000\n",
            "Loss: 9905795.000000\n",
            "Loss: 9901118.000000\n",
            "Loss: 9897585.000000\n",
            "Loss: 9895398.000000\n",
            "Loss: 9894709.000000\n",
            "Loss: 9893067.000000\n",
            "Loss: 9891664.000000\n",
            "Loss: 9889049.000000\n",
            "Loss: 9886371.000000\n",
            "Loss: 9884740.000000\n",
            "Loss: 9882573.000000\n",
            "Loss: 9880644.000000\n",
            "Loss: 9878593.000000\n",
            "Loss: 9875242.000000\n",
            "Loss: 9873677.000000\n",
            "Loss: 9870545.000000\n",
            "Loss: 9868929.000000\n",
            "Loss: 9867155.000000\n",
            "Loss: 9864342.000000\n",
            "Loss: 9861358.000000\n",
            "Loss: 9859498.000000\n",
            "Loss: 9857011.000000\n",
            "Loss: 9853890.000000\n",
            "Loss: 9850687.000000\n",
            "Loss: 9848332.000000\n",
            "Loss: 9846854.000000\n",
            "Loss: 9844441.000000\n",
            "Epoch: 24/30\n",
            "Loss: 9841435.000000\n",
            "Loss: 9839116.000000\n",
            "Loss: 9837029.000000\n",
            "Loss: 9833762.000000\n",
            "Loss: 9831984.000000\n",
            "Loss: 9829747.000000\n",
            "Loss: 9828542.000000\n",
            "Loss: 9825841.000000\n",
            "Loss: 9823250.000000\n",
            "Loss: 9819976.000000\n",
            "Loss: 9817897.000000\n",
            "Loss: 9816251.000000\n",
            "Loss: 9814570.000000\n",
            "Loss: 9812806.000000\n",
            "Loss: 9809737.000000\n",
            "Loss: 9808314.000000\n",
            "Loss: 9806594.000000\n",
            "Loss: 9805319.000000\n",
            "Loss: 9803854.000000\n",
            "Loss: 9801183.000000\n",
            "Loss: 9798704.000000\n",
            "Loss: 9795576.000000\n",
            "Loss: 9793836.000000\n",
            "Loss: 9792040.000000\n",
            "Loss: 9788853.000000\n",
            "Loss: 9785794.000000\n",
            "Loss: 9783867.000000\n",
            "Loss: 9782730.000000\n",
            "Loss: 9779311.000000\n",
            "Loss: 9776717.000000\n",
            "Epoch: 25/30\n",
            "Loss: 9773997.000000\n",
            "Loss: 9772675.000000\n",
            "Loss: 9771324.000000\n",
            "Loss: 9770035.000000\n",
            "Loss: 9767495.000000\n",
            "Loss: 9765729.000000\n",
            "Loss: 9763701.000000\n",
            "Loss: 9761649.000000\n",
            "Loss: 9759806.000000\n",
            "Loss: 9757837.000000\n",
            "Loss: 9755571.000000\n",
            "Loss: 9753385.000000\n",
            "Loss: 9750889.000000\n",
            "Loss: 9748752.000000\n",
            "Loss: 9746525.000000\n",
            "Loss: 9745203.000000\n",
            "Loss: 9743145.000000\n",
            "Loss: 9741542.000000\n",
            "Loss: 9739984.000000\n",
            "Loss: 9737861.000000\n",
            "Loss: 9736482.000000\n",
            "Loss: 9733121.000000\n",
            "Loss: 9730499.000000\n",
            "Loss: 9728453.000000\n",
            "Loss: 9726501.000000\n",
            "Loss: 9724682.000000\n",
            "Loss: 9722549.000000\n",
            "Loss: 9720577.000000\n",
            "Loss: 9718578.000000\n",
            "Loss: 9717173.000000\n",
            "Epoch: 26/30\n",
            "Loss: 9715039.000000\n",
            "Loss: 9713032.000000\n",
            "Loss: 9710585.000000\n",
            "Loss: 9708923.000000\n",
            "Loss: 9707396.000000\n",
            "Loss: 9705801.000000\n",
            "Loss: 9702500.000000\n",
            "Loss: 9701584.000000\n",
            "Loss: 9700017.000000\n",
            "Loss: 9699108.000000\n",
            "Loss: 9697796.000000\n",
            "Loss: 9695795.000000\n",
            "Loss: 9700144.000000\n",
            "Loss: 9692749.000000\n",
            "Loss: 9691999.000000\n",
            "Loss: 9690391.000000\n",
            "Loss: 9688761.000000\n",
            "Loss: 9687453.000000\n",
            "Loss: 9684994.000000\n",
            "Loss: 9684330.000000\n",
            "Loss: 9682786.000000\n",
            "Loss: 9681059.000000\n",
            "Loss: 9679293.000000\n",
            "Loss: 9677367.000000\n",
            "Loss: 9676167.000000\n",
            "Loss: 9674558.000000\n",
            "Loss: 9672830.000000\n",
            "Loss: 9670755.000000\n",
            "Loss: 9669587.000000\n",
            "Loss: 9668164.000000\n",
            "Epoch: 27/30\n",
            "Loss: 9666458.000000\n",
            "Loss: 9664196.000000\n",
            "Loss: 9662350.000000\n",
            "Loss: 9660287.000000\n",
            "Loss: 9658924.000000\n",
            "Loss: 9656906.000000\n",
            "Loss: 9654033.000000\n",
            "Loss: 9651809.000000\n",
            "Loss: 9650272.000000\n",
            "Loss: 9648983.000000\n",
            "Loss: 9648024.000000\n",
            "Loss: 9646782.000000\n",
            "Loss: 9644562.000000\n",
            "Loss: 9643251.000000\n",
            "Loss: 9641786.000000\n",
            "Loss: 9639663.000000\n",
            "Loss: 9638133.000000\n",
            "Loss: 9635319.000000\n",
            "Loss: 9633078.000000\n",
            "Loss: 9631443.000000\n",
            "Loss: 9629879.000000\n",
            "Loss: 9628091.000000\n",
            "Loss: 9625968.000000\n",
            "Loss: 9624233.000000\n",
            "Loss: 9622811.000000\n",
            "Loss: 9621525.000000\n",
            "Loss: 9620213.000000\n",
            "Loss: 9618843.000000\n",
            "Loss: 9616817.000000\n",
            "Loss: 9614810.000000\n",
            "Epoch: 28/30\n",
            "Loss: 9613474.000000\n",
            "Loss: 9612489.000000\n",
            "Loss: 9611126.000000\n",
            "Loss: 9609555.000000\n",
            "Loss: 9607182.000000\n",
            "Loss: 9607019.000000\n",
            "Loss: 9604964.000000\n",
            "Loss: 9604226.000000\n",
            "Loss: 9602879.000000\n",
            "Loss: 9601134.000000\n",
            "Loss: 9598406.000000\n",
            "Loss: 9595913.000000\n",
            "Loss: 9593534.000000\n",
            "Loss: 9592116.000000\n",
            "Loss: 9591106.000000\n",
            "Loss: 9589042.000000\n",
            "Loss: 9588408.000000\n",
            "Loss: 9585667.000000\n",
            "Loss: 9584891.000000\n",
            "Loss: 9583975.000000\n",
            "Loss: 9582931.000000\n",
            "Loss: 9580806.000000\n",
            "Loss: 9579462.000000\n",
            "Loss: 9577937.000000\n",
            "Loss: 9576543.000000\n",
            "Loss: 9574811.000000\n",
            "Loss: 9573572.000000\n",
            "Loss: 9571845.000000\n",
            "Loss: 9570446.000000\n",
            "Loss: 9569146.000000\n",
            "Epoch: 29/30\n",
            "Loss: 9567216.000000\n",
            "Loss: 9565710.000000\n",
            "Loss: 9564313.000000\n",
            "Loss: 9563616.000000\n",
            "Loss: 9562763.000000\n",
            "Loss: 9561574.000000\n",
            "Loss: 9559808.000000\n",
            "Loss: 9557430.000000\n",
            "Loss: 9558852.000000\n",
            "Loss: 9554193.000000\n",
            "Loss: 9553433.000000\n",
            "Loss: 9551949.000000\n",
            "Loss: 9551392.000000\n",
            "Loss: 9550022.000000\n",
            "Loss: 9548995.000000\n",
            "Loss: 9548083.000000\n",
            "Loss: 9546995.000000\n",
            "Loss: 9545413.000000\n",
            "Loss: 9543562.000000\n",
            "Loss: 9542239.000000\n",
            "Loss: 9541304.000000\n",
            "Loss: 9539787.000000\n",
            "Loss: 9538843.000000\n",
            "Loss: 9537345.000000\n",
            "Loss: 9536448.000000\n",
            "Loss: 9534935.000000\n",
            "Loss: 9533755.000000\n",
            "Loss: 9532535.000000\n",
            "Loss: 9531376.000000\n",
            "Loss: 9530190.000000\n",
            "Epoch: 30/30\n",
            "Loss: 9528899.000000\n",
            "Loss: 9528007.000000\n",
            "Loss: 9526644.000000\n",
            "Loss: 9525513.000000\n",
            "Loss: 9524535.000000\n",
            "Loss: 9523497.000000\n",
            "Loss: 9521508.000000\n",
            "Loss: 9519207.000000\n",
            "Loss: 9518234.000000\n",
            "Loss: 9516610.000000\n",
            "Loss: 9515838.000000\n",
            "Loss: 9514371.000000\n",
            "Loss: 9512619.000000\n",
            "Loss: 9510880.000000\n",
            "Loss: 9509703.000000\n",
            "Loss: 9508897.000000\n",
            "Loss: 9508055.000000\n",
            "Loss: 9506886.000000\n",
            "Loss: 9505591.000000\n",
            "Loss: 9504259.000000\n",
            "Loss: 9503096.000000\n",
            "Loss: 9501477.000000\n",
            "Loss: 9500912.000000\n",
            "Loss: 9500194.000000\n",
            "Loss: 9498898.000000\n",
            "Loss: 9496857.000000\n",
            "Loss: 9495574.000000\n",
            "Loss: 9493239.000000\n",
            "Loss: 9492301.000000\n",
            "Loss: 9491255.000000\n",
            "Style transfer is completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9lX7TV55gq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}